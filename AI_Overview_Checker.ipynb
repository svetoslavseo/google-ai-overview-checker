{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqxZhbx4yN9/i2miv0Nobo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetoslavseo/google-ai-overview-checker/blob/main/AI_Overview_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhRExo-yF5e8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import csv\n",
        "\n",
        "def load_keywords_from_csv(file_path):\n",
        "    keywords = []\n",
        "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            if row:  # Ensure the row is not empty\n",
        "                keywords.append(row[0])\n",
        "    return keywords\n",
        "\n",
        "def get_serp_results(api_username, api_password, keywords, location_code=2826, language_code='en'):\n",
        "    url = \"https://api.dataforseo.com/v3/serp/google/organic/live/advanced\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        payload = [{\n",
        "            \"keyword\": keyword,\n",
        "            \"location_code\": location_code,\n",
        "            \"language_code\": language_code,\n",
        "            \"device\": \"mobile\",\n",
        "            \"os\": \"android\",\n",
        "            \"depth\": 100,\n",
        "            \"load_async_ai_overview\": True\n",
        "        }]\n",
        "\n",
        "        print(f\"Sending request for: {keyword}\")  # Debugging info\n",
        "        print(json.dumps(payload, indent=4))\n",
        "\n",
        "        response = requests.post(url, auth=(api_username, api_password), json=payload)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            results[keyword] = extract_ai_overview_references(data)\n",
        "        else:\n",
        "            print(f\"Error fetching data for {keyword}: {response.status_code} {response.text}\")\n",
        "            results[keyword] = None\n",
        "\n",
        "        time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "    return results\n",
        "\n",
        "def extract_ai_overview_references(data):\n",
        "    try:\n",
        "        serp_data = data.get('tasks', [])[0].get('result', [])\n",
        "        if serp_data:\n",
        "            for result in serp_data[0].get('items', []):\n",
        "                if result.get('type') == 'ai_overview':\n",
        "                    references = result.get('references', [])\n",
        "                    extracted_references = []\n",
        "                    for ref in references:\n",
        "                        extracted_references.append([\n",
        "                            ref.get('source', ''),\n",
        "                            ref.get('domain', ''),\n",
        "                            ref.get('url', ''),\n",
        "                            ref.get('title', ''),\n",
        "                            ref.get('text', '')\n",
        "                        ])\n",
        "                    return extracted_references\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing response: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_results_to_csv(results, output_file):\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Keyword\", \"Source\", \"Domain\", \"URL\", \"Title\", \"Text\"])\n",
        "        for keyword, references in results.items():\n",
        "            if references:\n",
        "                for ref in references:\n",
        "                    writer.writerow([keyword] + ref)\n",
        "            else:\n",
        "                writer.writerow([keyword, \"No AI Overview\", \"\", \"\", \"\", \"\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    api_username = \"email\" # paste you DataForSEO email here\n",
        "    api_password = \"password\" # paste your pass here\n",
        "    keywords_file = \"/Users/petkovs/PycharmProjects/pythonProject/keywords - Sheet1.csv\"  # Path to CSV file containing keywords\n",
        "    output_file = \"/Users/petkovs/PycharmProjects/pythonProject/ai_overview_results.csv\"  # Output CSV file\n",
        "\n",
        "    keywords = load_keywords_from_csv(keywords_file)\n",
        "\n",
        "    results = get_serp_results(api_username, api_password, keywords)\n",
        "\n",
        "    save_results_to_csv(results, output_file)\n",
        "\n",
        "    print(f\"Results saved to {output_file}\")"
      ]
    }
  ]
}